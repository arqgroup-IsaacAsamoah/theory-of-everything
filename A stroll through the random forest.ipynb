{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "The goal of this guide is introduce, or re-introduce you to the basics of a random forest. You won't find many formulas or formal algorithm definitions. I hope you will find a simple intuitive understanding of what a random forest is, how it works, why it works and how to build one in python. I also will provide references to further reading for those interested in digging deeper. \n",
    "\n",
    "We're going to start with a 30 second summary of what a random forest is, then break that down into the basic building blocks. Once we are familiar with the building blocks we will explore each of them in a little more detail. Finally we will put everything we've learned together by implementing the random forest algorithm in python.\n",
    "\n",
    "## 30 Second summary\n",
    "\n",
    "A random forest is made up of decision trees. A decision tree involves segementing the predictor space into simple regions. In order to make a prediction for a given observation, we take the mean (for regression) or the mode (for classification) of the training observations in the region to which it belongs. \n",
    "\n",
    "The set of rules used to define each region can be summarised as a tree, hence the name. A random forest grows many such trees and takes the mean or mode of predictions across the trees to achieve improved predictive performance compared to a sinlge decision tree.\n",
    "\n",
    "If this doesn't make sense yet, don't worry about it, it will make sense as we step through it in more detail in the following sections. We are going to grow a random forest for regression today, but the pricinples we will learn apply to clasiifcation as well. The only differences are terminal node calculations and cost function selection.\n",
    "\n",
    "## Building blocks\n",
    "\n",
    "We've learned a random forest is made up of decision trees.  How many trees are in the forest? The number of decision trees is determined by the user. Selecting the number of decision trees is important and often comes down to a simple cost-benefit equation: the cost of calculating more trees vs the possible benefit of  increased performance. \n",
    "\n",
    "To create multiple decision trees from the same training data, a technique referred to as bagging is used. Bagging is a common process in machine learning and is not limited to tree based methods.  Essentially bagging is building multiple models, each based on a sample of the  training data. \n",
    "\n",
    "To generate these samples, bootstrapping is used. To understand bootstrapping, imagine you have a tiny data set:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>c</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47</td>\n",
       "      <td>4</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53</td>\n",
       "      <td>7</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>39</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>21</td>\n",
       "      <td>7</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    a   c   y\n",
       "0  44   2  46\n",
       "1  47   4  37\n",
       "2  53   7  25\n",
       "3   0   6  77\n",
       "4   3   8  72\n",
       "5   3   8   9\n",
       "6  39  10  20\n",
       "7   9   1  69\n",
       "8  19   6  47\n",
       "9  21   7  64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "toy_data = pd.DataFrame({'a' : np.random.choice(57, 10), 'c' : np.random.choice(11, 10),\n",
    "                         'y' : np.random.choice(78, 10)})\n",
    "\n",
    "display(toy_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To boostrap this tiny data, you first need to randomly select one row. The row selected is our first observation sampled from `toy_data`. Next you sample another row, noting that the row you sampled first remains in the pool of possible rows to be selected. Repeat this process until you have the number of observations you would like. Now you have a bootstrapped sample. When growing a random forest the number of rows selected through bootstrapping will generally be eqaul to the number of rows in the training data. The number of bootstrapped samples you need is equal to the number of decision trees you need to grow.\n",
    "\n",
    "As you can see, bootstrapping is simply sampling with replacement from the training data, with the number of rows in each sample set to the number of rows in the training data and the number of samples set to the number of trees required for your forest.\n",
    "\n",
    "So we have a number of decision trees, grown based on bootstrapped samples of our training data. Do we have a random forest yet? Not quite. We need to address the random part of the random forest. In each stage of growing a decision trees in the random forest, all predictors are considered in order to determine which is best to determine the next step in the tree. In a random forest tree, at each of these stages a random sample of possible predictors is taken. This limits which predictors can be chosen for each stage. \n",
    "\n",
    "Why is this important? Imagine three trees grown with this modified process, compared to three trees grown with the standard process. The three random forest trees will most likely be less similar to each other, because they have each been forced to consider a randomly selected set of predictors. Each tree is likely considering predictors other trees have ignored. Compare this with the standard process - these tree will most likely be quite similar to each other. They have all considered the same set of predictors at each stage, the only difference is the bootstrapped sample they recieve as training input. \n",
    "\n",
    "The result of the random forest tree process is reduced correlation among trees.  A prediction in a random forest as simply a summary of the predictions from the the decision trees in the forest. The goal of summarising over many trees is to reduce variance. Summarising over a set of less correlated trees will reduce variance more than summarising over more correlated trees. Take home? A random forest will generally have less variance than just bagging a decision tree. Put another way, the random forest predictions will be more consistent even when the test data is quite different from the training data.\n",
    "\n",
    "Now that we have an understanding of the basic building blocks, let's explore them, one at a time.\n",
    "\n",
    "\n",
    "## Decision trees\n",
    "The most important component of a random forest is the decision tree. As we talked about earlier a random forest is simply a collection of decision trees with some extra rules about how the decision trees are grown. \n",
    "\n",
    "A decision tree is made up of nodes. Referring back to our 30 second summary, a node is a rule that helps create the regions defined by a decision tree. Let's explore this more with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>c</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47</td>\n",
       "      <td>4</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53</td>\n",
       "      <td>7</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>39</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>21</td>\n",
       "      <td>7</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    a   c   y\n",
       "0  44   2  46\n",
       "1  47   4  37\n",
       "2  53   7  25\n",
       "3   0   6  77\n",
       "4   3   8  72\n",
       "5   3   8   9\n",
       "6  39  10  20\n",
       "7   9   1  69\n",
       "8  19   6  47\n",
       "9  21   7  64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(toy_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use our toy data again. Imagine, a and c are predictors of y. To create a node, we need to select a predictor and a value to define a rule for this node. An example of such a rule would be \"a < 44\". A node with this rule would create two new data sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>c</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>39</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>21</td>\n",
       "      <td>7</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    a   c   y\n",
       "3   0   6  77\n",
       "4   3   8  72\n",
       "5   3   8   9\n",
       "6  39  10  20\n",
       "7   9   1  69\n",
       "8  19   6  47\n",
       "9  21   7  64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>c</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47</td>\n",
       "      <td>4</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53</td>\n",
       "      <td>7</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    a  c   y\n",
       "0  44  2  46\n",
       "1  47  4  37\n",
       "2  53  7  25"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(toy_data[toy_data['a'] < 44])\n",
    "display(toy_data[toy_data['a'] >= 44])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that is our root node, it splits the training data in two, based on its rule. To create a whole decision tree we simply repat this process on each of the \"branches\" until we reach a terminal node. \n",
    "\n",
    "But, how do we select the right rules for the nodes? We need some criteria. Let's think about the purpose of splitting the data. If the prediction we produce with the tree is the mean of the final split to which the obervation belongs,  we want the observations in the final split to be as similar as possible. That is, we want low variance. It follows that the criteria for selecting rules for each node needs to minimise var(y[p < s]) + var(y[p >= s]).\n",
    "\n",
    "Where y is the outcome variable, p is a predictor and s is a the value at whcih the predictor is split.\n",
    "\n",
    "Let's try this with our `toy_data`. First we define a function to calculate var(y[p < s]) + var(y[p >= s])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_RSS(y1, y2):\n",
    "    \n",
    "    '''calculate the combined residual sum of squares for y1 and y1'''\n",
    "    \n",
    "    return np.sum((y1 - np.mean(y1))**2) + np.sum((y2 - np.mean(y2))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty simple right? Just calculate the variance for each branch and add them together. Now we need to generate all the possible splits to assess with the critera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_splits(y, f):\n",
    "   \n",
    "    '''split the y variable by split s in feature f'''\n",
    "    \n",
    "    return ([{'feature' : f.name, 'split' :  s, 'left': y[f < s],\n",
    "              'right' :  y[f >= s]} for s in f])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a list of dictionaries for each feature, where each entry contains a feature, a split and the left and right branches created by the split. Next we need to calculate the variance for each possible splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(feature_split): \n",
    "    \n",
    "    '''calculate the total RSS for the splits defined by splitting feature f at split s'''\n",
    "    \n",
    "    return ({'feature' : feature_split['feature'],\n",
    "              'split' : feature_split['split'],\n",
    "              'cost' : combined_RSS(feature_split['left'], feature_split['right'])\n",
    "            }\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we need to select the feature split with the minimum variance, and that is our first node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random as rand\n",
    "\n",
    "def flatten_list(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "    \n",
    "def get_split_costs(df):\n",
    "    \n",
    "    '''calulate the cost for each split'''\n",
    "    \n",
    "    return flatten_list(\n",
    "        [[cost_function(split) for split in get_feature_splits(\n",
    "            df.iloc[:,-1], df.iloc[:,:-1].loc[:, feature])] for feature in rand.sample(list(\n",
    "            df.iloc[:,:-1].columns), int(math.sqrt(len(list(df.iloc[:,:-1].columns)))))])\n",
    "\n",
    "def select_node(split_costs): \n",
    "       \n",
    "    '''select the lowest cost node'''\n",
    "    \n",
    "    return (rand.choice([{'feature' : split['feature'], 'split' : split['split'],\n",
    "                          'cost' : split['cost']}\n",
    "                         for split in split_costs if split['cost'] == np.min(\n",
    "                             [split['cost'] for split in split_costs])]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a bit more going on here, let's break it down. The first function is just a helper to flatten lists. The second function `get_split_cost` executes `cost_function` on the all the  feature splits generated by `get_feature_splits`.  Because this is a random forest tree this function also limits the features at each split to a random sample. More on that later.\n",
    "\n",
    "Finally, `select_node` scans all the split varainces and selects the split with the lowest variance. And we have our first node. Let's run it on `toy_data` and see what we get.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature': 'c', 'split': 10, 'cost': 4268.222222222222}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "root_node = select_node(get_split_costs(toy_data))\n",
    "display(root_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the `toy_data`, the root node is defined by the rule a < 53 and the combined variances of the branches is 674.75. \n",
    "\n",
    "Once we have the root node, we simply repeat the above process on each branch until a terminal node is reached. What makes a node terminal? We need some more criteria. \n",
    "\n",
    "It turns out defining criteria for a terminal node is really simple. Tree depth or node size. Tree depth referes to the number of nodes in a tree, or how many times the training data has been split. Node size is the number of observations in that node. That is, the number of observations that satisfy the rule defining the node. \n",
    "\n",
    "So to define the terminal node, first we set a maximum tree depth, which controls tree complexity. The more complex the tree is, the more it is influenced by the training data. This is called variance. Generally we want to keep variance low, while also managing model bias. A bias model isn't influenced enough by the training data.  So we want a balance of the two - a model that is influenced just enough by the training data. \"Just enough\" is tricky to define. Mostly we want to glean as much information as we can from the training data, without following it too closely, because we want the model to work for other samples from our problem data space. Next we set a minnimum node size. Because we take the mean (or mode in classification) of the terminal node, we don't want the node sizr to be so small, the mean/mode is meaningless. Also if there is only one or two observations in our terminal nodes we have probably followed the training data too closely and the model will be highly variable.\n",
    "\n",
    "Ok, so no we now how to stop. How do we efficiently repeat the branching process? We use recursion. Recursion is just repitition, but it can be tricky because it is a *process that repeats itself*. So the logic can get a bit slippery. I'm not going to go into recursion deeply here, suffice to say its worth looking to if you want to build a decision tree. [Here](https://medium.com/@siddharthgupta555t/finally-understanding-recursion-and-binary-search-trees-857c85e72978) is a good  place to start your recursive journey.  Just to make it a little trickier, we will be using tree based recursion, because we are building a tree. The article above introduces you to general recursion and tree base recursion, so have a read of that and I will see you when you are done.\n",
    "\n",
    "So, you have crushed recursion (or have decided you don't care), let's grow a decision tree.\n",
    "\n",
    "A recursive process has two parts, the base case and the recursive case. The stopping criteria tells us which to apply. When the stoppig criteria is met, return the base case, if not, return the recursive case. Let's do it.\n",
    "\n",
    "First, lets write a helper function to handle the base case, its really simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def terminal_node(node_y):\n",
    "    \n",
    "    '''return the prediction for the terminal node'''\n",
    "    \n",
    "    return np.mean(node_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done. Return the mean. Now, the fun part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grow_tree(df, node, depth, max_depth = 1, min_size = 5 ):\n",
    "    \n",
    "    ''' recursively grow a decision tree by applying the function to each node it \n",
    "    returns until max depth of min size criteria is met '''\n",
    "    \n",
    "    left = df.loc[df.loc[:, node['feature']] < node['split']]\n",
    "    right = df.loc[df.loc[:, node['feature']] >= node['split']]\n",
    "    \n",
    "    if left.empty or right.empty:\n",
    "        return terminal_node(list(left.iloc[:, -1]) + list(right.iloc[:, -1]))\n",
    "        \n",
    "    elif depth >= max_depth:\n",
    "        return {'node': node, \n",
    "                'left': terminal_node(left.iloc[:, -1]), \n",
    "                'right': terminal_node(right.iloc[:, -1])}\n",
    "    \n",
    "    else:\n",
    "        return {'node' : node,\n",
    "                \n",
    "                'left' : (lambda x: terminal_node(list(x.iloc[:, -1])) \n",
    "                          if len(x.iloc[:, -1]) <= min_size\n",
    "                          else grow_tree(x, select_node(get_split_costs(x)),\n",
    "                                        depth + 1, max_depth, min_size))(left),\n",
    "                \n",
    "                'right' : (lambda x: terminal_node(list(x.iloc[:, -1])) \n",
    "                           if len(x.iloc[:, -1]) <= min_size \n",
    "                           else grow_tree(x, select_node(get_split_costs(x)),\n",
    "                                        depth + 1, max_depth, min_size))(right)\n",
    "               }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at this function step by step. The first statements are just naming the left and right node of the first split (one of the inputs to this function is the result of the `select_node` function.  Next is our first base case. I say first because we have 3 possible stopping conditions, which gives us three scenarios in which we return terminal node, which is our base case output.\n",
    "\n",
    "So our first stopping criteria is no split possible. We haven't discussed this yet, and it is pretty simple. When the split results in one branch rather than two. Just means  all the values of the input either satify the rule or do not satisfy it. So if that happens we return the mean of the node. Done.\n",
    "\n",
    "Next criteria is max depth. You can see we have depth and max depth as inputs, if depth is greater or equal to max depth, return the left and right nodes as terminal nodes.\n",
    "\n",
    "The third stopping criteria is embedded in the recursive case. This is an artifact of tree based recursion. The third criteria  applies to each of the branches created by the input node separately. We simply check the size of each node, if its less than or equal to the min size, return the terminal node for that branch.\n",
    "\n",
    "To understand why we apply the criteria separately, think about the case where the left branch  is less than the minnimum size but the right is not.\n",
    "\n",
    "So that is all of our base cases, on to the recursive case.\n",
    "\n",
    "The recursive case says \"If neither of the outer stopping criteria are met and the inner criteria is not met, return yourself with the depth increased by one.\" This is done for the left branch and the irhg branch separately. That's it. \n",
    "\n",
    "To understand why this approach works, let's work through one recursion. If you have read the recursion article referenced above, recall the dicussion about the call stack. That is the key to understanding why this function works. It was for me anyway.\n",
    "\n",
    "Imagine we call `grow_tree((toy_data, root_node, 1, 2,  3 )`.  Our first  layer in the stack is the this funcion. We evaluate the stopping criteria. left or right empty is false, max depth is false. Left min size is false, so we call `grow_tree((left, select_node(get_split_costs(left)), depth + 1, max_depth, min_size)` and this goes to the next layer of our stack.  So we execute this recursed function, left or right empty is false, but max depth is true, so we return the current node, with its left and right terminal nodes. That stack is popped and we move to the next step oin our base stack, the right branch, the same thing happens, we add a stack, and pop it, then we pop our bottom layer and return our 2 depth tree. \n",
    "\n",
    "Lets run it and look at the result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node': {'feature': 'c', 'split': 10, 'cost': 4268.222222222222},\n",
       " 'left': {'node': {'feature': 'a', 'split': 47, 'cost': 3382.8571428571427},\n",
       "  'left': 54.857142857142854,\n",
       "  'right': 31.0},\n",
       " 'right': 20.0}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grow_tree(toy_data, root_node, 1, 2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out our best first split only leaves us with one entry in the right branch, so the right branch in our example actually terminates without recursion, because it meets the minimum node size stopping criteria. And there we have it, a recursive function to grow a decision tree. We have our first building block: the decision tree, next let's bag this decision tree to give us a forest. \n",
    "\n",
    "## Bagging\n",
    "\n",
    "To bag a decision tree, bootstrapping is used. As discussed earlier bootstraping is imply re-sampling from the training data with replacement. Let's do it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(df, random_state):\n",
    "    return df.sample(len(df), replace = True, random_state = random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see I've used the `pandas` `sample` method. See docs [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sample.html/).\n",
    "\n",
    "That's all there is to it. Now to see it in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grow_random_forest(df, max_depth = 1, min_size = 5, no_of_trees = 10):  \n",
    "    return [grow_tree(\n",
    "        bootstrap(df, i), select_node( get_split_costs(bootstrap(df, i))),\n",
    "        1, max_depth, min_size) for i in range(0, no_of_trees)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To grow a forest, build trees from bootstrapped samples of the input data until you reach the desired number of trees. Done. Let's try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'node': {'feature': 'c', 'split': 7, 'cost': 4379.6},\n",
       "  'left': {'node': {'feature': 'c', 'split': 6, 'cost': 264.5},\n",
       "   'left': 57.5,\n",
       "   'right': 77.0},\n",
       "  'right': {'node': {'feature': 'a', 'split': 21, 'cost': 3406.5},\n",
       "   'left': 30.0,\n",
       "   'right': 44.5}},\n",
       " {'node': {'feature': 'c', 'split': 8, 'cost': 968.0952380952381},\n",
       "  'left': {'node': {'feature': 'a', 'split': 44, 'cost': 332.0},\n",
       "   'left': 61.0,\n",
       "   'right': 43.0},\n",
       "  'right': 12.666666666666666},\n",
       " {'node': {'feature': 'a', 'split': 39, 'cost': 2696.25},\n",
       "  'left': {'node': {'feature': 'a', 'split': 9, 'cost': 2347.5},\n",
       "   'left': 40.5,\n",
       "   'right': 52.5},\n",
       "  'right': {'node': {'feature': 'a', 'split': 47, 'cost': 96.0},\n",
       "   'left': 20.0,\n",
       "   'right': 29.0}}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grow_random_forest(toy_data, 2, 3, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have a forest of 3 trees with a max depth of 2 and a minimum node size of 3, based on `toy_data`.\n",
    "## Making it random\n",
    "The final building block is making the trees random forest trees. Recall that this means for each node we limited the available predictors to a random sample of the predictor set. And we have actually done this in our `get_split_costs` function earlier. Let's take a closer look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_costs(df):\n",
    "    \n",
    "    '''calulate the cost for each split'''\n",
    "    \n",
    "    return flatten_list(\n",
    "        [[cost_function(split) for split in get_feature_splits(\n",
    "            df.iloc[:,-1], df.iloc[:,:-1].loc[:, feature])] for feature in rand.sample(list(\n",
    "            df.iloc[:,:-1].columns), int(math.sqrt(len(list(df.iloc[:,:-1].columns)))))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last two lines of this function limit the predictors to be assesed to be those randomly selected from all predictors. The size of the sample is the square root of the total number of predictors.  The feature sample size in generals should be treated as a hyperparameter to be tuned. I have chosen the sqrt(p) here for convenience.\n",
    "\n",
    "And just like that you have a random forest. Let's put it all together.\n",
    "\n",
    "## Growing a random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_RSS(y1, y2):\n",
    "    \n",
    "    '''calculate the combined residual sum of squares for y1 and y1'''\n",
    "    \n",
    "    return np.sum((y1 - np.mean(y1))**2) + np.sum((y2 - np.mean(y2))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random as rand\n",
    "\n",
    "\n",
    "def get_feature_splits(y, f):\n",
    "   \n",
    "    '''split the y variable by split s in feature f'''\n",
    "    \n",
    "    return ([{'feature' : f.name, 'split' :  s, 'left': y[f < s],\n",
    "              'right' :  y[f >= s]} for s in f])\n",
    "\n",
    "\n",
    "def cost_function(feature_split): \n",
    "    \n",
    "    '''calculate the total RSS for the splits defined by splitting feature f at split s'''\n",
    "    \n",
    "    return ({'feature' : feature_split['feature'],\n",
    "              'split' : feature_split['split'],\n",
    "              'cost' : combined_RSS(feature_split['left'], feature_split['right'])\n",
    "            }\n",
    "           )\n",
    "\n",
    "\n",
    "def flatten_list(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "\n",
    "def get_split_costs(df):\n",
    "    \n",
    "    '''calulate the cost for each split'''\n",
    "    \n",
    "    return flatten_list(\n",
    "        [[cost_function(split) for split in get_feature_splits(\n",
    "            df.iloc[:,-1], df.iloc[:,:-1].loc[:, feature])] for feature in rand.sample(list(\n",
    "            df.iloc[:,:-1].columns), int(math.sqrt(len(list(df.iloc[:,:-1].columns)))))])\n",
    "\n",
    "\n",
    "def select_node(split_costs): \n",
    "       \n",
    "    '''select the lowest cost node'''\n",
    "    \n",
    "    return (rand.choice([{'feature' : split['feature'], 'split' : split['split'],\n",
    "                          'cost' : split['cost']}\n",
    "                         for split in split_costs if split['cost'] == np.min(\n",
    "                             [split['cost'] for split in split_costs])]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Growing a tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def terminal_node(node_y):\n",
    "    \n",
    "    '''return the prediction for the terminal node'''\n",
    "    \n",
    "    return np.mean(node_y)\n",
    "\n",
    "\n",
    "def grow_tree(df, node, depth, max_depth = 1, min_size = 5 ):\n",
    "    \n",
    "    ''' recursively grow a decision tree by applying the function to each node it \n",
    "    returns until max depth of min size criteria is met '''\n",
    "    \n",
    "    left = df.loc[df.loc[:, node['feature']] < node['split']]\n",
    "    right = df.loc[df.loc[:, node['feature']] >= node['split']]\n",
    "    \n",
    "    if left.empty or right.empty:\n",
    "        return terminal_node(list(left.iloc[:, -1]) + list(right.iloc[:, -1]))\n",
    "        \n",
    "    elif depth >= max_depth:\n",
    "        return {'node': node, \n",
    "                'left': terminal_node(left.iloc[:, -1]), \n",
    "                'right': terminal_node(right.iloc[:, -1])}\n",
    "    \n",
    "    else:\n",
    "        return {'node' : node,\n",
    "                \n",
    "                'left' : (lambda x: terminal_node(list(x.iloc[:, -1])) \n",
    "                          if len(x.iloc[:, -1]) <= min_size\n",
    "                          else grow_tree(x, select_node(get_split_costs(x)),\n",
    "                                        depth + 1, max_depth, min_size))(left),\n",
    "                \n",
    "                'right' : (lambda x: terminal_node(list(x.iloc[:, -1])) \n",
    "                           if len(x.iloc[:, -1]) <= min_size \n",
    "                           else grow_tree(x, select_node(get_split_costs(x)),\n",
    "                                        depth + 1, max_depth, min_size))(right)\n",
    "               }\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Growing a forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(df, random_state):\n",
    "    \n",
    "    ''' return a random sample of the same size as the df, with replacement'''\n",
    "    \n",
    "    return df.sample(len(df), replace = True, random_state = random_state)\n",
    "\n",
    "\n",
    "def grow_random_forest(df, max_depth = 1, min_size = 5, no_of_trees = 10):  \n",
    "    \n",
    "    ''' return a specified number of random forest decision trees, \n",
    "    each based on a bootstrapped sample of df '''\n",
    "    \n",
    "    return [grow_tree(\n",
    "        bootstrap(df, i), select_node( get_split_costs(bootstrap(df, i))),\n",
    "        1, max_depth, min_size) for i in range(0, no_of_trees)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap up\n",
    "\n",
    "So that is one way to implement the random forest algorithm.  For more intel on the theory see:  [Introduction to Stastical Learning](https://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf) and [Elements of Statistical Learning](https://web.stanford.edu/~hastie/Papers/ESLII.pdf). \n",
    "\n",
    "For completeness I've included some code to make predictions and some output performance metrics and an example with the sklearn boston dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_predict_row(row, tree):\n",
    "    \n",
    "    '''use the trained tree to partition the feature space of the test observation and return \n",
    "    the partition prediction value, which for regression is the mean of the partition'''\n",
    "    \n",
    "    if row[tree['node']['feature']] <  tree['node']['split']:\n",
    "        if not isinstance(tree['left'], dict):\n",
    "            return tree['left']\n",
    "        else:\n",
    "            return tree_predict_row(row, tree['left'])\n",
    "    else:\n",
    "         if not isinstance(tree['right'], dict):\n",
    "            return tree['right']\n",
    "         else:\n",
    "            return tree_predict_row(row, tree['right'])\n",
    "        \n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forest_predict_row(row, forest):\n",
    "    \n",
    "    ''' predicts each row of df for each tree than takes the average across \n",
    "    all trees for each row '''\n",
    "    \n",
    "    return np.mean([tree_predict_row(row, tree) for tree in forest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(df, forest):\n",
    "\n",
    "    ''' applies predict row function to a df of test observations '''\n",
    "    \n",
    "    return df.apply(forest_predict_row, axis = 1, forest = forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y, predicted_y):\n",
    "    \n",
    "    ''' returns the mean square error: the sum of squyared differences between predicted y and actual y'''\n",
    "    \n",
    "    return np.sum((y - predicted_y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_sum_of_squares(y):\n",
    "\n",
    "    '''returns the total sum of squares: the sum of squared difference between y and the mean of y i.e var(y)'''\n",
    "    \n",
    "    return np.sum((y - np.mean(y))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_squared(y, predicted_y):\n",
    "   \n",
    "    ''' retruns to proportion of variance attributable the model: 1 - the ratio of mse to total sum of squares.'''\n",
    "\n",
    "    return 1 - (mse(y, predicted_y)/total_sum_of_squares(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "# training data\n",
    "y = datasets.load_boston()['target']\n",
    "X = datasets.load_boston()['data']\n",
    "columns = datasets.load_boston()['feature_names']\n",
    "data = pd.DataFrame(X)\n",
    "data.columns = columns\n",
    "data.loc[:, 'y'] = y\n",
    "\n",
    "train = data.sample(frac =0.7)\n",
    "test = data[~data.isin(train)].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_forest = grow_random_forest(train, max_depth = 5, min_size = 5, no_of_trees = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'node': {'feature': 'LSTAT', 'split': 9.74, 'cost': 14469.636662539735},\n",
       "  'left': {'node': {'feature': 'LSTAT',\n",
       "    'split': 4.67,\n",
       "    'cost': 5883.217384219553},\n",
       "   'left': {'node': {'feature': 'AGE',\n",
       "     'split': 49.7,\n",
       "     'cost': 1882.7201503759397},\n",
       "    'left': {'node': {'feature': 'DIS',\n",
       "      'split': 7.3967,\n",
       "      'cost': 926.0085714285715},\n",
       "     'left': {'node': {'feature': 'ZN',\n",
       "       'split': 80.0,\n",
       "       'cost': 188.50000000000003},\n",
       "      'left': 34.49999999999999,\n",
       "      'right': 50.0},\n",
       "     'right': 30.0},\n",
       "    'right': {'node': {'feature': 'RM',\n",
       "      'split': 6.943,\n",
       "      'cost': 186.7923076923076},\n",
       "     'left': 22.8,\n",
       "     'right': {'node': {'feature': 'B',\n",
       "       'split': 385.05,\n",
       "       'cost': 135.09999999999997},\n",
       "      'left': 47.6,\n",
       "      'right': 43.6}}},\n",
       "   'right': {'node': {'feature': 'CRIM',\n",
       "     'split': 9.2323,\n",
       "     'cost': 3026.8851428571434},\n",
       "    'left': {'node': {'feature': 'RM',\n",
       "      'split': 6.842,\n",
       "      'cost': 1452.8326352067868},\n",
       "     'left': {'node': {'feature': 'LSTAT',\n",
       "       'split': 9.38,\n",
       "       'cost': 902.2163619909502},\n",
       "      'left': 23.693846153846156,\n",
       "      'right': 27.03529411764706},\n",
       "     'right': {'node': {'feature': 'DIS', 'split': 2.8617, 'cost': 288.67475},\n",
       "      'left': 36.7625,\n",
       "      'right': 32.14}},\n",
       "    'right': 50.0}},\n",
       "  'right': {'node': {'feature': 'NOX',\n",
       "    'split': 0.668,\n",
       "    'cost': 2858.423287615974},\n",
       "   'left': {'node': {'feature': 'B',\n",
       "     'split': 372.75,\n",
       "     'cost': 1633.1998412698413},\n",
       "    'left': {'node': {'feature': 'TAX',\n",
       "      'split': 307.0,\n",
       "      'cost': 491.2800000000001},\n",
       "     'left': 22.1,\n",
       "     'right': {'node': {'feature': 'PTRATIO',\n",
       "       'split': 18.8,\n",
       "       'cost': 328.8233846153847},\n",
       "      'left': 21.82,\n",
       "      'right': 16.66923076923077}},\n",
       "    'right': {'node': {'feature': 'RAD',\n",
       "      'split': 5.0,\n",
       "      'cost': 851.6010169491526},\n",
       "     'left': {'node': {'feature': 'DIS',\n",
       "       'split': 2.0063,\n",
       "       'cost': 273.22519444444447},\n",
       "      'left': 16.0125,\n",
       "      'right': 19.53111111111111},\n",
       "     'right': {'node': {'feature': 'NOX',\n",
       "       'split': 0.614,\n",
       "       'cost': 322.41345454545467},\n",
       "      'left': 21.210909090909084,\n",
       "      'right': 28.0}}},\n",
       "   'right': {'node': {'feature': 'LSTAT',\n",
       "     'split': 18.05,\n",
       "     'cost': 579.9690056285178},\n",
       "    'left': {'node': {'feature': 'DIS',\n",
       "      'split': 2.5052,\n",
       "      'cost': 308.82746153846153},\n",
       "     'left': {'node': {'feature': 'PTRATIO',\n",
       "       'split': 20.2,\n",
       "       'cost': 167.58692810457512},\n",
       "      'left': 15.644444444444446,\n",
       "      'right': 15.182352941176472},\n",
       "     'right': {'node': {'feature': 'DIS',\n",
       "       'split': 2.5671,\n",
       "       'cost': 54.015454545454546},\n",
       "      'left': 22.150000000000002,\n",
       "      'right': 16.736363636363635}},\n",
       "    'right': {'node': {'feature': 'CRIM',\n",
       "      'split': 9.91655,\n",
       "      'cost': 105.05857142857144},\n",
       "     'left': {'node': {'feature': 'LSTAT',\n",
       "       'split': 21.52,\n",
       "       'cost': 22.24625000000001},\n",
       "      'left': 14.325000000000001,\n",
       "      'right': 11.9625},\n",
       "     'right': {'node': {'feature': 'DIS',\n",
       "       'split': 1.5004,\n",
       "       'cost': 22.31416666666666},\n",
       "      'left': 13.45,\n",
       "      'right': 8.291666666666666}}}}},\n",
       " {'node': {'feature': 'RM', 'split': 6.874, 'cost': 14151.73745925926},\n",
       "  'left': {'node': {'feature': 'LSTAT',\n",
       "    'split': 14.33,\n",
       "    'cost': 4787.384323408807},\n",
       "   'left': {'node': {'feature': 'AGE',\n",
       "     'split': 36.6,\n",
       "     'cost': 2385.4184869721475},\n",
       "    'left': {'node': {'feature': 'TAX',\n",
       "      'split': 300.0,\n",
       "      'cost': 637.7278591954023},\n",
       "     'left': {'node': {'feature': 'B',\n",
       "       'split': 396.33,\n",
       "       'cost': 221.16419580419588},\n",
       "      'left': 28.44545454545455,\n",
       "      'right': 24.584615384615383},\n",
       "     'right': {'node': {'feature': 'NOX',\n",
       "       'split': 0.504,\n",
       "       'cost': 281.5407407407407},\n",
       "      'left': 22.318518518518523,\n",
       "      'right': 27.3}},\n",
       "    'right': {'node': {'feature': 'LSTAT',\n",
       "      'split': 8.94,\n",
       "      'cost': 1291.06831299041},\n",
       "     'left': {'node': {'feature': 'RAD',\n",
       "       'split': 8.0,\n",
       "       'cost': 540.3704615384615},\n",
       "      'left': 23.39230769230769,\n",
       "      'right': 32.059999999999995},\n",
       "     'right': {'node': {'feature': 'AGE',\n",
       "       'split': 100.0,\n",
       "       'cost': 396.3512328767124},\n",
       "      'left': 21.31095890410959,\n",
       "      'right': 15.0}}},\n",
       "   'right': {'node': {'feature': 'LSTAT',\n",
       "     'split': 19.69,\n",
       "     'cost': 1614.6336780866193},\n",
       "    'left': {'node': {'feature': 'PTRATIO',\n",
       "      'split': 19.7,\n",
       "      'cost': 653.2827604166666},\n",
       "     'left': {'node': {'feature': 'TAX',\n",
       "       'split': 330.0,\n",
       "       'cost': 196.08500000000006},\n",
       "      'left': 22.077777777777776,\n",
       "      'right': 18.794444444444444},\n",
       "     'right': {'node': {'feature': 'B',\n",
       "       'split': 376.88,\n",
       "       'cost': 295.930294117647},\n",
       "      'left': 14.049999999999995,\n",
       "      'right': 16.511764705882353}},\n",
       "    'right': {'node': {'feature': 'NOX',\n",
       "      'split': 0.538,\n",
       "      'cost': 304.68126086956516},\n",
       "     'left': 19.22,\n",
       "     'right': {'node': {'feature': 'NOX',\n",
       "       'split': 0.679,\n",
       "       'cost': 186.90251521298177},\n",
       "      'left': 13.494117647058824,\n",
       "      'right': 10.375862068965516}}}},\n",
       "  'right': {'node': {'feature': 'RM',\n",
       "    'split': 7.47,\n",
       "    'cost': 1900.2530952380953},\n",
       "   'left': {'node': {'feature': 'PTRATIO',\n",
       "     'split': 20.2,\n",
       "     'cost': 843.2494117647059},\n",
       "    'left': {'node': {'feature': 'PTRATIO',\n",
       "      'split': 17.9,\n",
       "      'cost': 453.1239153439153},\n",
       "     'left': {'node': {'feature': 'INDUS',\n",
       "       'split': 3.41,\n",
       "       'cost': 372.3390909090909},\n",
       "      'left': 34.090909090909086,\n",
       "      'right': 30.724999999999998},\n",
       "     'right': {'node': {'feature': 'RAD',\n",
       "       'split': 7.0,\n",
       "       'cost': 0.8720000000000063},\n",
       "      'left': 35.46,\n",
       "      'right': 33.4}},\n",
       "    'right': {'node': {'feature': 'LSTAT',\n",
       "      'split': 13.44,\n",
       "      'cost': 57.152000000000015},\n",
       "     'left': 28.2,\n",
       "     'right': 15.76}},\n",
       "   'right': {'node': {'feature': 'RM',\n",
       "     'split': 7.489,\n",
       "     'cost': 41.61600000000003},\n",
       "    'left': 43.5,\n",
       "    'right': {'node': {'feature': 'DIS',\n",
       "      'split': 5.2119,\n",
       "      'cost': 33.843809523809554},\n",
       "     'left': {'node': {'feature': 'RAD',\n",
       "       'split': 8.0,\n",
       "       'cost': 13.520000000000016},\n",
       "      'left': 50.0,\n",
       "      'right': 47.4},\n",
       "     'right': 47.333333333333336}}}},\n",
       " {'node': {'feature': 'RM', 'split': 6.951, 'cost': 16322.17381683696},\n",
       "  'left': {'node': {'feature': 'CRIM',\n",
       "    'split': 9.32909,\n",
       "    'cost': 8360.350424710427},\n",
       "   'left': {'node': {'feature': 'DIS',\n",
       "     'split': 1.3216,\n",
       "     'cost': 5996.653151750973},\n",
       "    'left': 50.0,\n",
       "    'right': {'node': {'feature': 'LSTAT',\n",
       "      'split': 10.19,\n",
       "      'cost': 3408.9526546489565},\n",
       "     'left': {'node': {'feature': 'RM',\n",
       "       'split': 6.144,\n",
       "       'cost': 1553.7208296296294},\n",
       "      'left': 21.537037037037038,\n",
       "      'right': 25.794666666666664},\n",
       "     'right': {'node': {'feature': 'LSTAT',\n",
       "       'split': 16.14,\n",
       "       'cost': 1007.8349399038459},\n",
       "      'left': 19.66923076923077,\n",
       "      'right': 16.0671875}}},\n",
       "   'right': {'node': {'feature': 'NOX',\n",
       "     'split': 0.659,\n",
       "     'cost': 310.7246464646465},\n",
       "    'left': {'node': {'feature': 'NOX',\n",
       "      'split': 0.631,\n",
       "      'cost': 83.82875000000003},\n",
       "     'left': {'node': {'feature': 'DIS',\n",
       "       'split': 2.8237,\n",
       "       'cost': 45.06833333333334},\n",
       "      'left': 15.016666666666667,\n",
       "      'right': 20.1},\n",
       "     'right': 23.1},\n",
       "    'right': {'node': {'feature': 'DIS',\n",
       "      'split': 2.0651,\n",
       "      'cost': 117.71483870967744},\n",
       "     'left': {'node': {'feature': 'B',\n",
       "       'split': 240.52,\n",
       "       'cost': 100.3988655462185},\n",
       "      'left': 8.87857142857143,\n",
       "      'right': 10.170588235294119},\n",
       "     'right': 15.600000000000001}}},\n",
       "  'right': {'node': {'feature': 'PTRATIO', 'split': 20.2, 'cost': 3045.636125},\n",
       "   'left': {'node': {'feature': 'CRIM', 'split': 2.01019, 'cost': 2568.068},\n",
       "    'left': {'node': {'feature': 'CHAS',\n",
       "      'split': 1.0,\n",
       "      'cost': 2396.3915116279068},\n",
       "     'left': {'node': {'feature': 'LSTAT',\n",
       "       'split': 4.73,\n",
       "       'cost': 1678.3265999999999},\n",
       "      'left': 42.843999999999994,\n",
       "      'right': 34.68333333333334},\n",
       "     'right': 29.950000000000003},\n",
       "    'right': 50.0},\n",
       "   'right': 20.619999999999997}},\n",
       " {'node': {'feature': 'LSTAT', 'split': 8.94, 'cost': 15440.062868617326},\n",
       "  'left': {'node': {'feature': 'RM', 'split': 7.47, 'cost': 4700.922647058824},\n",
       "   'left': {'node': {'feature': 'LSTAT',\n",
       "     'split': 8.88,\n",
       "     'cost': 2449.1882352941175},\n",
       "    'left': {'node': {'feature': 'RM',\n",
       "      'split': 6.826,\n",
       "      'cost': 1277.1502063679245},\n",
       "     'left': {'node': {'feature': 'RM',\n",
       "       'split': 6.43,\n",
       "       'cost': 471.631927536232},\n",
       "      'left': 22.208695652173912,\n",
       "      'right': 26.656666666666673},\n",
       "     'right': {'node': {'feature': 'PTRATIO',\n",
       "       'split': 20.2,\n",
       "       'cost': 329.46873563218384},\n",
       "      'left': 33.231034482758616,\n",
       "      'right': 24.266666666666666}},\n",
       "    'right': 50.0},\n",
       "   'right': {'node': {'feature': 'NOX',\n",
       "     'split': 0.718,\n",
       "     'cost': 236.65999999999997},\n",
       "    'left': {'node': {'feature': 'CRIM',\n",
       "      'split': 0.12083,\n",
       "      'cost': 108.76399999999994},\n",
       "     'left': {'node': {'feature': 'CHAS', 'split': 1.0, 'cost': 0.0},\n",
       "      'left': 50.0,\n",
       "      'right': 50.0},\n",
       "     'right': {'node': {'feature': 'TAX',\n",
       "       'split': 403.0,\n",
       "       'cost': 23.499999999999957},\n",
       "      'left': 42.7,\n",
       "      'right': 50.0}},\n",
       "    'right': 21.9}},\n",
       "  'right': {'node': {'feature': 'PTRATIO',\n",
       "    'split': 20.2,\n",
       "    'cost': 5406.58117156165},\n",
       "   'left': {'node': {'feature': 'RM',\n",
       "     'split': 5.67,\n",
       "     'cost': 1137.5179411764707},\n",
       "    'left': {'node': {'feature': 'NOX', 'split': 0.544, 'cost': 47.144},\n",
       "     'left': {'node': {'feature': 'ZN',\n",
       "       'split': 12.5,\n",
       "       'cost': 15.147999999999998},\n",
       "      'left': 20.22,\n",
       "      'right': 17.4},\n",
       "     'right': {'node': {'feature': 'RM',\n",
       "       'split': 4.973,\n",
       "       'cost': 5.908750000000002},\n",
       "      'left': 11.8,\n",
       "      'right': 15.2875}},\n",
       "    'right': {'node': {'feature': 'PTRATIO',\n",
       "      'split': 14.7,\n",
       "      'cost': 810.8371000000002},\n",
       "     'left': 31.0,\n",
       "     'right': {'node': {'feature': 'INDUS',\n",
       "       'split': 2.89,\n",
       "       'cost': 590.3911111111112},\n",
       "      'left': 36.2,\n",
       "      'right': 21.27777777777778}}},\n",
       "   'right': {'node': {'feature': 'CRIM',\n",
       "     'split': 15.1772,\n",
       "     'cost': 3283.8626548672573},\n",
       "    'left': {'node': {'feature': 'LSTAT',\n",
       "      'split': 15.17,\n",
       "      'cost': 2072.745320754717},\n",
       "     'left': {'node': {'feature': 'NOX',\n",
       "       'split': 0.631,\n",
       "       'cost': 949.9990701754386},\n",
       "      'left': 18.99736842105263,\n",
       "      'right': 23.406666666666663},\n",
       "     'right': {'node': {'feature': 'DIS',\n",
       "       'split': 1.5004,\n",
       "       'cost': 731.3230508474576},\n",
       "      'left': 27.5,\n",
       "      'right': 13.883050847457627}},\n",
       "    'right': {'node': {'feature': 'B',\n",
       "      'split': 9.32,\n",
       "      'cost': 108.91749999999999},\n",
       "     'left': 15.0,\n",
       "     'right': {'node': {'feature': 'DIS',\n",
       "       'split': 1.5888,\n",
       "       'cost': 28.814999999999998},\n",
       "      'left': 12.325,\n",
       "      'right': 7.85}}}}},\n",
       " {'node': {'feature': 'RM', 'split': 7.47, 'cost': 13635.828761094226},\n",
       "  'left': {'node': {'feature': 'LSTAT',\n",
       "    'split': 9.68,\n",
       "    'cost': 6283.072076523311},\n",
       "   'left': {'node': {'feature': 'LSTAT',\n",
       "     'split': 5.49,\n",
       "     'cost': 1874.5239832285122},\n",
       "    'left': {'node': {'feature': 'NOX',\n",
       "      'split': 0.403,\n",
       "      'cost': 358.1430303030303},\n",
       "     'left': 23.166666666666668,\n",
       "     'right': {'node': {'feature': 'LSTAT',\n",
       "       'split': 5.08,\n",
       "       'cost': 247.67578260869556},\n",
       "      'left': 32.53913043478261,\n",
       "      'right': 28.57}},\n",
       "    'right': {'node': {'feature': 'RM',\n",
       "      'split': 7.079,\n",
       "      'cost': 1007.679405940594},\n",
       "     'left': {'node': {'feature': 'NOX',\n",
       "       'split': 0.605,\n",
       "       'cost': 902.4912439613527},\n",
       "      'left': 23.953260869565216,\n",
       "      'right': 27.15555555555555},\n",
       "     'right': 32.5}},\n",
       "   'right': {'node': {'feature': 'NOX',\n",
       "     'split': 0.668,\n",
       "     'cost': 2151.538424129353},\n",
       "    'left': {'node': {'feature': 'RM',\n",
       "      'split': 5.602,\n",
       "      'cost': 991.936574074074},\n",
       "     'left': {'node': {'feature': 'AGE',\n",
       "       'split': 92.7,\n",
       "       'cost': 20.70514285714285},\n",
       "      'left': 18.557142857142857,\n",
       "      'right': 13.620000000000001},\n",
       "     'right': {'node': {'feature': 'PTRATIO',\n",
       "       'split': 21.0,\n",
       "       'cost': 730.4642663219132},\n",
       "      'left': 20.459340659340658,\n",
       "      'right': 17.01764705882353}},\n",
       "    'right': {'node': {'feature': 'LSTAT',\n",
       "      'split': 19.77,\n",
       "      'cost': 543.0683974358975},\n",
       "     'left': {'node': {'feature': 'RM',\n",
       "       'split': 6.152,\n",
       "       'cost': 341.0107407407408},\n",
       "      'left': 18.049999999999997,\n",
       "      'right': 15.68148148148148},\n",
       "     'right': {'node': {'feature': 'RM',\n",
       "       'split': 4.519,\n",
       "       'cost': 116.87958333333336},\n",
       "      'left': 13.8,\n",
       "      'right': 10.445833333333333}}}},\n",
       "  'right': {'node': {'feature': 'DIS',\n",
       "    'split': 1.9709,\n",
       "    'cost': 237.96956521739128},\n",
       "   'left': 21.9,\n",
       "   'right': {'node': {'feature': 'B',\n",
       "     'split': 396.9,\n",
       "     'cost': 151.4781818181818},\n",
       "    'left': {'node': {'feature': 'PTRATIO',\n",
       "      'split': 14.9,\n",
       "      'cost': 110.27816666666668},\n",
       "     'left': {'node': {'feature': 'INDUS',\n",
       "       'split': 3.97,\n",
       "       'cost': 35.208333333333336},\n",
       "      'left': 50.0,\n",
       "      'right': 48.916666666666664},\n",
       "     'right': {'node': {'feature': 'B',\n",
       "       'split': 392.63,\n",
       "       'cost': 44.48874999999998},\n",
       "      'left': 45.8875,\n",
       "      'right': 50.0}},\n",
       "    'right': 38.7}}}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(boston_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_predictions = predict(test, boston_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3088.0329382836526"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8029567917599614"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(mse(test.iloc[:, -1], boston_predictions))\n",
    "display(r_squared(test.iloc[:, -1],boston_predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
