{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "A random forest is made up of decision trees. A decision involves segementing the predictor space into simple regions. In order to make a prediction for a given observation, we take the mean (for regression) or the mode (for classification) of the training observations in the region to which it belongs. \n",
    "\n",
    "The set of rules used to define each region can be summarised as a tree, hence the name. A random forest grows many such trees and takes the mean or mode of predictions across the trees to achieve improve predictive performance compared to a sinlge decision tree.\n",
    "\n",
    "If this all makes sense, you can safely jump to the implementation section. If not, don't worry about it, we will step through each part of the random forest in the following sections.\n",
    "\n",
    "## Introducing some terminology\n",
    "\n",
    "We've learned a random forest is made up of decision trees.  How many trees are in the forest? The number of decision trees is determined by the user. Selecting the number of decision trees is important and often comes down to a simple cost-benefit equation: the cost of calculating more trees vs the possible benefit of  increased performance. \n",
    "\n",
    "To create multiple decision trees from the same training data, a technique referred to as bagging is used. Bagging is a common process in machine learning and is not limited to tree based methods.  Essentially bagging is building multiple models, each based on a sample of the  training data. \n",
    "\n",
    "To generate these samples, bootstrapping is used. To understand bootstrapping, imagine you have a tiny data set:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>c</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47</td>\n",
       "      <td>7</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    a  c   y\n",
       "0  44  3  36\n",
       "1  47  7  70\n",
       "2  53  9  12\n",
       "3   0  3  58\n",
       "4   3  5  65"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "toy_data = pd.DataFrame({'a' : np.random.choice(57, 5), 'c' : np.random.choice(11, 5),\n",
    "                         'y' : np.random.choice(78, 5)})\n",
    "\n",
    "display(toy_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To boostrap this tiny data, you first need to randomly select one row. The row selected is our first observation sampled from `toy_data`. Next you sample another row, noting that the row you sampled first remains in the pool of possible rows to be selected. Repeat this process until you have the number of observations you would like. Now you have a bootstrapped sample. When growing a random forest the number of rows selected through bootstrapping will generally be eqaul to the number of rows in the training data. The number of bootstrapped samples you need is equal to the number of decision trees you need to grow.\n",
    "\n",
    "As you can see, bootstrapping is simply sampling with replacement from the training data, with the number of rows in each sample set to the number of rows in the training data and the number of samples set to the number of trees required for your forest.\n",
    "\n",
    "So we have a number of decision trees, grown based on bootstrapped samples of our training data. Do we have a random forest yet? Not quite. We need to address the random part of the random forest. In each stage of growing a decision trees in the random forest, all features are considered in order to determine which is best to determine the next step in the tree. In a random forest tree, at each of these stages a random sample of possible features is taken. This limits which features can be chosen for each stage. \n",
    "\n",
    "Why is this important? Imagine three trees grown with this modified process, compared to three trees grown with the standard process. The three random forest trees will most likely be less similar to each other, because they have each been forced to consider a ramdonly selected set of features. Each tree is likely considering features other trees have ignored. Compare this with the standard process - these tree will most likely be quite similar to each other. They have all considered the same set of features at each stage, the only difference is the bootstrapped sample they recieve as training input. \n",
    "\n",
    "The result of the random forest tree process is reduced correletion between trees.  A prediction in a random forest is simply a summary of the predictions from the the decision trees in the forest. The goal of summarising over many trees is to reduce variance. \n",
    "\n",
    "\n",
    "\n",
    "If we are predicting a contimuous outcome, the random forest predcition is the mean of the tree predictions. If we are predicting s categorical outcome, the random forest prediction is the category most frequently selected by the trees. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from src import i_kit_learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "y = datasets.load_boston()['target']\n",
    "X = datasets.load_boston()['data']\n",
    "columns = datasets.load_boston()['feature_names']\n",
    "data = pd.DataFrame(X)\n",
    "data.columns = columns\n",
    "data.loc[:, 'y'] = y\n",
    "\n",
    "train = data.sample(frac =0.7)\n",
    "test = data[~data.isin(train)].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_forest = i_kit_learn.grow_random_forest(train,max_depth = 5, min_size = 5, no_of_trees = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1948.9641726721798"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8398139414492283"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "boston_predictions = i_kit_learn.predict(test, boston_forest)\n",
    "display(i_kit_learn.mse(test.iloc[:, -1], boston_predictions))\n",
    "display(i_kit_learn.r_squared(test.iloc[:, -1], boston_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
